Colt Steele's Udemy Course: JavaScript Algorithms and Data Structures Masterclass

BIG O NOTATION

What's the idea here?
    - Imagine we have multiple implementations (ways to do) of the same function
    - How can we determine which one is the "best"?

Who cares?
    - It's important to have a precise vocabulary to talk about how our code performs
    - Useful for discussing trade-offs between different approaches
    - When your code slows down or crashes, identifying parts of the code that are inefficient can help us find pain points in our applications
    - It comes up in interviews!

What does "best" or "better" mean? It's referring to which implementation/method to solve the problem is:
    - faster
    - less memory-intensive

To test performance why not use timers? there's a problem with this
    - different machines will record different times
    - the same machine will record different times
    - for fast algorithms, speed measurements may not be precise enough?

If not time, then what?
    - rather than counting seconds, which are so variable... let's count the number of simple operations the computer has to perform!

Counting operations is hard!
    - depending on what we count, the number of operations can be as low as 2n or as high as 5n + 2. but regardless of the exact number, the number of operations grows roughly proportionally with n

Introducing...Big O
    - Big O Notation is a way to formalize fuzzy counting
    - It allows us to talk formally about how the runtime of an algorithm grows as the inputs grow
    - We won't care about the details, only the trends

Big O Definition
    - We say that an algorithm is big O of f of n - O(f(n)) - if the number of simple operations the computer has to do is eventually less than a constant times f of n - f(n) - as n increases
        -f(n) could be linear where (f(n) = n) meaning as n (input) grows run time is increasing just as much
        - f(n) could be quadratic where (f(n) = n²) meaning as n (input) grows runtime is increasing to the square of n
        - f(n) could be constant where (f(n) = 1) meaning as n (input) grows it doesn't really have an affect on runtime
        - f(n) could be something entirely different! 

**Remember: big O notation is just a generalized way for talking about how efficient an algorithm is as n (an input) grows, how does that affect change or reflect in runtime? All we care about is the general trend. Not counting out each of the operations going on.**

Simplifying Big O Expressions
    - when determining the time complexity of an algorithm, there are some helpful rules of thumb for big O expressions.
        - constants don't matter.
        if a function were to have O(2n) you could simplify that to O(n). likewise, if a function had a O(500) you could simplify that to O(1) its how we say something is constant. if a function had a 0(13n²) you could simplify to O(n²)

Big O Shorthands
    - analyzing complexity with big O can get complicated
    there are several rules that can help
    - these rules don't ALWAYS work, but they are a helpful starting point
        1. Arithmetic operations are constant. so if we're adding something or subtracting or dividing and so on thats going to be constant runtime.
        2. Variable assignment is constant x equaling any number is about the same amount of runtime
        3. Accessing elements in an array (by index) or (by key) is constant
        4. In a loop, the complexity is the length of the loop times the complexity of whatever happens inside of the loop

Space Complexity
    - So far, we've been focusing on time complexity: how can we analyze the runtime of an algorithm as the size of the inputs increases?
    - But we can also use big O notation to analyze SPACE COMPLEXITY, how much additional memory do we need to allocate in order to run the code in our algorithm?

What about the inputs?
    - Sometimes you'll hear the term auxiliary space complexity to refer to space required by the algorithm, not including space taken up by inputs.
    - Unless otherwise noted, when we talk about space complexity, technically we'll be talking about auxiliary space complexity.

Space Complexity in JS 
    - Rules of Thumb
        - most primitives (booleans, numbers, undefined, null) are constant space
        - strings require O(n) space (where n is the string length)
        - reference types are generally O(n), where n is the length (for arrays) or the number of keys (for objects)

Logarithms
    - We've encountered some of the most common complexities: O(1), O(n), O(n²)
    - Sometimes big O expressions involve more complex mathematical expressions
    - one that appears more often than you might like is the logarithm!

Wait, what's a logarithm?
    a logarithm is the inverse of exponentiation.
    - so we may see log₂(8) = 3 and its saying 2 to the what power equals 8. aka log₂(value) = exponent
    - when writing big 0 notation for logarithms we're going to omit the 2, because like for time and space complexity, we care about the general trend

Huh?
    - this isn't a math course so here's a rule of thumb.
    - the logarithm of a number roughly measures the number of times you can divide that number by 2 before you get a value that is less than or equal to 1.
    Example: 8/2 = 4, 4/2 = 2, 2/2 = 1; we divided three times. so the log(8) = 3

In general, logarithm time complexity is great! the runtime is short. you can see better visually on the trends graph

Who cares?
    - certain searching algorithms have logarithmic time complexity
    - efficient sorting algorithms involve logarithms
    - recursion sometimes involve logarithmic space complexity

**Recap
    - to analyze the performance of an algorithm, we use Big O Notation
    - Big O Notation can give us a high level understanding of the time or space complexity of an algorithm
    - Big O Notation doesn't care about precision, only about general trends (linear? quadratic? constant?)
    - The time or space complexity (as measured by Big O) depends only on the algorithm, not the hardware used to run the algorithm 
    - Big O is everywhere, so get lots of practice!**

ANALYZING PERFORMANCE OF ARRAYS AND OBJECTS

Objects include unordered, key value pairs!

When to use objects
    - when you don't need order
    - when you need fast access / insertion and removal

Big O of Objects
    - Insertion - O(1)
    - Removal - O(1)
    - Searching - O(n)
    - Access - O(1)

Big O of Object Methods
    - Object.keys - O(n)
    - Object.values - O(n)
    - Object.entries - O(n)
    - hasOwnProperty - O(1)

Arrays are ordered lists!

When to use objects
    - when you need order
    - when you need fast access / insertion and removal (sort of...)

Big O of Arrays
    - Insertion - it depends...
        -inserting at the end of an array would have a O(1) one operation essentially, constant
        - inserting at the beginning of an array would have a O(n) because in addition to inserting whatever into the beginning of the array, it also has to re-index each element in the array
    - Removal - it depends...
        - same premise goes for removal
        -removing at the end of an array would have a O(1) one operation essentially, constant
        - removing at the beginning of an array would have a O(n) because in addition to removing whatever into the beginning of the array, it also has to re-index each element in the array
    - Searching - O(n)
    - Access - O(1)

Big O of Array Operations
    - push - O(1)
    - pop - O(1)
    - shift - O(n)
    - unshift - O(n)
    - concat - O(n)
    - slice - O(n)
    - splice - O(n)
    - sort - O(n * log n)
    - forEach/map/filter/reduce/etc. - O(n)

PROBLEM-SOLVING APPROACH

What is an algorithm?
    - a process or set of steps to accomplish a certain task

Why do I need to know this?
    - Almost everything that you do in programming involves some kind of algorithm!
    - It's the foundation for being a successful problem solving and developer
    - Also... INTERVIEWS

How do you improve?
    1. Devise a plan for solving problems
    2. Master common problem solving patterns

Step 1: Understand the Problem
    Ask yourself these questions:
        1. Can I restate the problem in my own words?
        2. What are the inputs that go into the problem?
        3. What are the outputs that should come from the solution to the problem?
        4. Can the outputs be determined from the inputs? In other words, do I have enough information to solve the problem? (You may not be able to answer this question until you set about solving the problem. That's okay; it's still worth considering the question at this early stage.)
        5. How should I label the important pieces of data that are a part of the problem?

Step 2: Explore Concrete Examples
    - Coming up with examples can help you understand the problem better
    - Examples also provide sanity checks that your eventual solution works how it should
        - Start with Simple Examples
        - Progress to More Complex Examples
        - Explore Examples with Empty Inputs
        - Explore Examples with Invalid Inputs
    - Thinking like this and understanding these edge cases helps you build a more robust solution, something that is more foolproof.

Step 3: Break it Down
    - Take the actual steps of the problem and write them down. Doesn't need to be full pseudocode or valid syntax, just comments as a guide for steps to take is good enough. Especially in an interview its important. You don't want to start typing right away or start writing code on a whiteboard silently. It's better to say, "All right, here's the steps I'm going to try and take."
    - aka EXPLICITLY WRITE OUT THE STEPS YOU NEED TO TAKE
    -This forces you to think about the code you'll write before you write it, and helps you catch any lingering conceptual issues or misunderstandings before you dive in and have to worry about details (e.g. language syntax) as well.

Step 4: Solve or Simplify
    - Solve the problem if you can. If you can't, solve a simpler problem!
    - Simplify
        - Find the core difficulty in what you're trying to do
        - Temporarily ignore that difficulty
        - Write a simplified solution
        - Then incorporate that difficulty back in 

Step 5: Look Back and Refactor
    - Congrats on solving it, but you're not done!
    - Can you check the result? Making sure the code works.
    - Can you derive the result differently? Is there other ways or approaches to solve it.
    - Can you understand it at a glance? How intuitive is your code? Could it make sense on a paper/whiteboard?
    - Can you use the result or method for some other problem? It helps you find similarities in other problems.
    - Can you improve the performance of your solution? Think time and space complexity.
    - Can you think of other ways to refactor?
    - How have other people solved this problem?

Recap and Interview Strategies
    1. Understand the Problem: ask questions of interviewer, clarify the problem. Think about how solution should operate and behave in each scenario.

    2. Explore Concrete Examples

    3. Break It Down: write your process/gameplan out so you can show interviewer in what direction you were going

    4. Solve/Simplify: solve what you can if you can
    
    5. Look Back and Refactor: make sure its working, and think about if there are other ways the problem could have been solved

PROBLEM SOLVING PATTERNS
    
How Do You Improve?
    1. Devise a plan for solving problems
    
    2. Master common problem solving patterns
         Some patterns:
            - frequency counter
            - multiple pointers
            - sliding window
            - divide and conquer
            - dynamic programming
            - greedy algorithms
            - backtracking
            - many more!

Frequency Counter Pattern

    - this pattern uses objects or sets to collect values/frequencies of values
    - this can often avoid the need for nested loops or O(n²) operations with arrays/strings
    - this is useful in algorithms and challenges when you have multiple pieces of data, and you need to compare them to see if they consist of similar values, if they are anagrams of one another, if a value is contained inside of another value--any time you're comparing pieces of data to input(s) and frequencies of them occurring

Multiple Pointers Pattern
    - creating pointers or values that correspond to an index or position and move towards the beginning, end or middle based on a certain condition
    - very efficient for solving problems with minimal space complexity as well
    - we have some sort of structure like a string or array and the idea is to search for something that meets a condition (usually a pair) and we use two references. we'll start one reference maybe at the beginning and one at the end and work towards each other(towards the middle), or one reference could start at the first letter and the second references start at the second letter and work towards the end in the same direction, and vice versa if started references at the end of the string or array  

Sliding Window Pattern
    - This pattern involves creating a window which can either be an array or number from one position to another
    - Depending on a certain condition, the window either increases or closes (and a new window is created)
    - Very useful for keeping track of a subset of data in an array/string etc.

Divide and Conquer Pattern
    - This pattern involves dividing a data set into smaller chunks and then repeating a process with a subset of data. 
    - This pattern can tremendously decrease time complexity
    - We take a larger set of data, usually an array, string, could be a linked list or tree. And we start by dividing it into smaller pieces and then doing something to each smaller piece to determine where to go next.
