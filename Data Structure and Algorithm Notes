Colt Steele's Udemy Course: JavaScript Algorithms and Data Structures Masterclass

BIG O NOTATION

What's the idea here?
    - Imagine we have multiple implementations (ways to do) of the same function
    - How can we determine which one is the "best"?

Who cares?
    - It's important to have a precise vocabulary to talk about how our code performs
    - Useful for discussing trade-offs between different approaches
    - When your code slows down or crashes, identifying parts of the code that are inefficient can help us find pain points in our applications
    - It comes up in interviews!

What does "best" or "better" mean? It's referring to which implementation/method to solve the problem is:
    - faster
    - less memory-intensive

To test performance why not use timers? there's a problem with this
    - different machines will record different times
    - the same machine will record different times
    - for fast algorithms, speed measurements may not be precise enough?

If not time, then what?
    - rather than counting seconds, which are so variable... let's count the number of simple operations the computer has to perform!

Counting operations is hard!
    - depending on what we count, the number of operations can be as low as 2n or as high as 5n + 2. but regardless of the exact number, the number of operations grows roughly proportionally with n

Introducing...Big O
    - Big O Notation is a way to formalize fuzzy counting
    - It allows us to talk formally about how the runtime of an algorithm grows as the inputs grow
    - We won't care about the details, only the trends

Big O Definition
    - We say that an algorithm is big O of f of n - O(f(n)) - if the number of simple operations the computer has to do is eventually less than a constant times f of n - f(n) - as n increases
        -f(n) could be linear where (f(n) = n) meaning as n (input) grows run time is increasing just as much
        - f(n) could be quadratic where (f(n) = n²) meaning as n (input) grows runtime is increasing to the square of n
        - f(n) could be constant where (f(n) = 1) meaning as n (input) grows it doesn't really have an affect on runtime
        - f(n) could be something entirely different! 

**Remember: big O notation is just a generalized way for talking about how efficient an algorithm is as n (an input) grows, how does that affect change or reflect in runtime? All we care about is the general trend. Not counting out each of the operations going on.**

Simplifying Big O Expressions
    - when determining the time complexity of an algorithm, there are some helpful rules of thumb for big O expressions.
        - constants don't matter.
        if a function were to have O(2n) you could simplify that to O(n). likewise, if a function had a O(500) you could simplify that to O(1) its how we say something is constant. if a function had a 0(13n²) you could simplify to O(n²)

Big O Shorthands
    - analyzing complexity with big O can get complicated
    there are several rules that can help
    - these rules don't ALWAYS work, but they are a helpful starting point
        1. Arithmetic operations are constant. so if we're adding something or subtracting or dividing and so on thats going to be constant runtime.
        2. Variable assignment is constant x equaling any number is about the same amount of runtime
        3. Accessing elements in an array (by index) or (by key) is constant
        4. In a loop, the complexity is the length of the loop times the complexity of whatever happens inside of the loop

Space Complexity
    - So far, we've been focusing on time complexity: how can we analyze the runtime of an algorithm as the size of the inputs increases?
    - But we can also use big O notation to analyze SPACE COMPLEXITY, how much additional memory do we need to allocate in order to run the code in our algorithm?

What about the inputs?
    - Sometimes you'll hear the term auxiliary space complexity to refer to space required by the algorithm, not including space taken up by inputs.
    - Unless otherwise noted, when we talk about space complexity, technically we'll be talking about auxiliary space complexity.

Space Complexity in JS 
    - Rules of Thumb
        - most primitives (booleans, numbers, undefined, null) are constant space
        - strings require O(n) space (where n is the string length)
        - reference types are generally O(n), where n is the length (for arrays) or the number of keys (for objects)

Logarithms
    - We've encountered some of the most common complexities: O(1), O(n), O(n²)
    - Sometimes big O expressions involve more complex mathematical expressions
    - one that appears more often than you might like is the logarithm!

Wait, what's a logarithm?
    a logarithm is the inverse of exponentiation.
    - so we may see log₂(8) = 3 and its saying 2 to the what power equals 8. aka log₂(value) = exponent
    - when writing big 0 notation for logarithms we're going to omit the 2, because like for time and space complexity, we care about the general trend

Huh?
    - this isn't a math course so here's a rule of thumb.
    - the logarithm of a number roughly measures the number of times you can divide that number by 2 before you get a value that is less than or equal to 1.
    Example: 8/2 = 4, 4/2 = 2, 2/2 = 1; we divided three times. so the log(8) = 3

In general, logarithm time complexity is great! the runtime is short. you can see better visually on the trends graph

Who cares?
    - certain searching algorithms have logarithmic time complexity
    - efficient sorting algorithms involve logarithms
    - recursion sometimes involve logarithmic space complexity

**Recap
    - to analyze the performance of an algorithm, we use Big O Notation
    - Big O Notation can give us a high level understanding of the time or space complexity of an algorithm
    - Big O Notation doesn't care about precision, only about general trends (linear? quadratic? constant?)
    - The time or space complexity (as measured by Big O) depends only on the algorithm, not the hardware used to run the algorithm 
    - Big O is everywhere, so get lots of practice!**

ANALYZING PERFORMANCE OF ARRAYS AND OBJECTS

Objects include unordered, key value pairs!

When to use objects
    - when you don't need order
    - when you need fast access / insertion and removal

Big O of Objects
    - Insertion - O(1)
    - Removal - O(1)
    - Searching - O(n)
    - Access - O(1)

Big O of Object Methods
    - Object.keys - O(n)
    - Object.values - O(n)
    - Object.entries - O(n)
    - hasOwnProperty - O(1)

Arrays are ordered lists!

When to use objects
    - when you need order
    - when you need fast access / insertion and removal (sort of...)

Big O of Arrays
    - Insertion - it depends...
        -inserting at the end of an array would have a O(1) one operation essentially, constant
        - inserting at the beginning of an array would have a O(n) because in addition to inserting whatever into the beginning of the array, it also has to re-index each element in the array
    - Removal - it depends...
        - same premise goes for removal
        -removing at the end of an array would have a O(1) one operation essentially, constant
        - removing at the beginning of an array would have a O(n) because in addition to removing whatever into the beginning of the array, it also has to re-index each element in the array
    - Searching - O(n)
    - Access - O(1)

Big O of Array Operations
    - push - O(1)
    - pop - O(1)
    - shift - O(n)
    - unshift - O(n)
    - concat - O(n)
    - slice - O(n)
    - splice - O(n)
    - sort - O(n * log n)
    - forEach/map/filter/reduce/etc. - O(n)


