Colt Steele's Udemy Course: JavaScript Algorithms and Data Structures Masterclass

BIG O NOTATION

What's the idea here?
    - Imagine we have multiple implementations (ways to do) of the same function
    - How can we determine which one is the "best"?

Who cares?
    - It's important to have a precise vocabulary to talk about how our code performs
    - Useful for discussing trade-offs between different approaches
    - When your code slows down or crashes, identifying parts of the code that are inefficient can help us find pain points in our applications
    - It comes up in interviews!

What does "best" or "better" mean? It's referring to which implementation/method to solve the problem is:
    - faster
    - less memory-intensive

To test performance why not use timers? there's a problem with this
    - different machines will record different times
    - the same machine will record different times
    - for fast algorithms, speed measurements may not be precise enough?

If not time, then what?
    - rather than counting seconds, which are so variable... let's count the number of simple operations the computer has to perform!

Counting operations is hard!
    - depending on what we count, the number of operations can be as low as 2n or as high as 5n + 2. but regardless of the exact number, the number of operations grows roughly proportionally with n

Introducing...Big O
    - Big O Notation is a way to formalize fuzzy counting
    - It allows us to talk formally about how the runtime of an algorithm grows as the inputs grow
    - We won't care about the details, only the trends

Big O Definition
    - We say that an algorithm is big O of f of n - O(f(n)) - if the number of simple operations the computer has to do is eventually less than a constant times f of n - f(n) - as n increases
        -f(n) could be linear where (f(n) = n) meaning as n (input) grows run time is increasing just as much
        - f(n) could be quadratic where (f(n) = nÂ²) meaning as n (input) grows runtime is increasing to the square of n
        - f(n) could be constant where (f(n) = 1) meaning as n (input) grows it doesn't really have an affect on runtime
        - f(n) could be something entirely different! 

REMEMBER: big O notation is just a generalized way for talking about how efficient an algorithm is as n (an input) grows, how does that affect change or reflect in runtime?